{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950307be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Amharic NER processing pipeline...\n",
      "✅ Prepared 50 samples at ..\\data\\labeled\\labeled_data.csv\n",
      "✅ Successfully created CONLL file at ../CoNLL/amharic_ner.conll\n",
      "Total tokens: 1607\n",
      "\n",
      "Pipeline completed successfully!\n",
      "\n",
      "Final outputs:\n",
      "- Labeled data: ..\\data\\labeled\\labeled_data.csv\n",
      "- CONLL format: ../CoNLL/amharic_ner.conll\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "class AmharicNERProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the processor with file paths\"\"\"\n",
    "        # Set up paths using pathlib\n",
    "        self.base_dir = Path('../data')\n",
    "        self.processed_path = self.base_dir / 'processed/cleaned_messages.csv'\n",
    "        self.labeling_path = self.base_dir / 'labeled/labeled_data.csv'\n",
    "        self.conll_path = '../CoNLL/amharic_ner.conll'\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        (self.base_dir / 'labeled').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def tokenize_amharic(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize Amharic text while preserving special characters\n",
    "        Args:\n",
    "            text (str): Amharic text to tokenize\n",
    "        Returns:\n",
    "            list: List of tokens\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return []\n",
    "            \n",
    "        # Tokenize Amharic words and keep punctuation as separate tokens\n",
    "        tokens = re.findall(r'[\\w\\u1200-\\u137F]+|[^\\w\\s]', text)\n",
    "        return [token.strip() for token in tokens if token.strip()]\n",
    "\n",
    "    def detect_entities(self, tokens):\n",
    "        \"\"\"\n",
    "        Auto-detect potential entities to assist with labeling\n",
    "        Args:\n",
    "            tokens (list): List of tokenized words\n",
    "        Returns:\n",
    "            list: List of labels with initial entity detection\n",
    "        \"\"\"\n",
    "        labels = ['O'] * len(tokens)\n",
    "        \n",
    "        # Price patterns (e.g., \"500 ብር\")\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == 'ብር' and i > 0 and tokens[i-1].isdigit():\n",
    "                labels[i-1] = 'B-PRICE'\n",
    "                labels[i] = 'I-PRICE'\n",
    "            elif token.startswith('ዋጋ፦'):\n",
    "                labels[i] = 'B-PRICE'\n",
    "        \n",
    "        # Product patterns\n",
    "        product_keywords = ['ልብስ', 'ስልክ', 'መጽሐፍ', 'ኮምፒዩተር', 'ወተት', 'ጠርሙስ']\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in product_keywords:\n",
    "                labels[i] = 'B-PRODUCT'\n",
    "                # Mark following related words\n",
    "                j = i + 1\n",
    "                while j < len(tokens) and labels[j] == 'O' and not tokens[j].isdigit():\n",
    "                    labels[j] = 'I-PRODUCT'\n",
    "                    j += 1\n",
    "        \n",
    "        # Location patterns\n",
    "        location_keywords = ['አድራሻ', 'መገናኛ', 'ቢሮ', 'ፎቅ', 'ከተማ', 'ደፋር']\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in location_keywords:\n",
    "                labels[i] = 'B-LOC'\n",
    "                # Mark address components\n",
    "                j = i + 1\n",
    "                while j < len(tokens) and labels[j] == 'O' and not tokens[j].isdigit():\n",
    "                    labels[j] = 'I-LOC'\n",
    "                    j += 1\n",
    "        \n",
    "        return labels\n",
    "\n",
    "    def prepare_labeling_data(self, sample_size=50):\n",
    "        \"\"\"\n",
    "        Prepare data for labeling with automatic entity detection\n",
    "        Args:\n",
    "            sample_size (int): Number of samples to prepare\n",
    "        Returns:\n",
    "            bool: True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load and filter data\n",
    "            df = pd.read_csv(self.processed_path)\n",
    "            df = df[df['amharic_text'].str.len() > 20].copy()\n",
    "            \n",
    "            if len(df) < sample_size:\n",
    "                print(f\"Warning: Only {len(df)} samples available\")\n",
    "                sample_size = len(df)\n",
    "            \n",
    "            # Stratified sampling by channel\n",
    "            samples = []\n",
    "            for channel in df['channel'].unique():\n",
    "                channel_samples = df[df['channel'] == channel].sample(frac=0.3, random_state=42)\n",
    "                samples.extend(channel_samples.to_dict('records'))\n",
    "                if len(samples) >= sample_size:\n",
    "                    break\n",
    "            \n",
    "            # Prepare labeling data with auto-detection\n",
    "            labeling_data = []\n",
    "            for sample in samples[:sample_size]:\n",
    "                tokens = self.tokenize_amharic(sample['amharic_text'])\n",
    "                labels = self.detect_entities(tokens)\n",
    "                \n",
    "                labeling_data.append({\n",
    "                    'original_text': sample['amharic_text'],\n",
    "                    'channel': sample['channel'],\n",
    "                    'message_id': sample['message_id'],\n",
    "                    'tokens': tokens,\n",
    "                    'labels': labels\n",
    "                })\n",
    "            \n",
    "            # Save to CSV\n",
    "            pd.DataFrame([{\n",
    "                'original_text': item['original_text'],\n",
    "                'tokens': str(item['tokens']),\n",
    "                'labels': str(item['labels'])\n",
    "            } for item in labeling_data]).to_csv(self.labeling_path, index=False)\n",
    "            \n",
    "            print(f\"✅ Prepared {len(labeling_data)} samples at {self.labeling_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error preparing labeling data: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def convert_to_conll(self):\n",
    "        \"\"\"\n",
    "        Convert labeled data to CONLL format\n",
    "        Returns:\n",
    "            bool: True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load labeled data\n",
    "            df = pd.read_csv(self.labeling_path)\n",
    "            \n",
    "            # Convert to CONLL format\n",
    "            conll_lines = []\n",
    "            for _, row in df.iterrows():\n",
    "                try:\n",
    "                    tokens = ast.literal_eval(row['tokens'])\n",
    "                    labels = ast.literal_eval(row['labels'])\n",
    "                    \n",
    "                    if len(tokens) != len(labels):\n",
    "                        continue\n",
    "                        \n",
    "                    for token, label in zip(tokens, labels):\n",
    "                        conll_lines.append(f\"{token}\\t{label}\")\n",
    "                    conll_lines.append(\"\")  # Empty line between sentences\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Save CONLL file\n",
    "            with open(self.conll_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"\\n\".join(conll_lines))\n",
    "            \n",
    "            print(f\"✅ Successfully created CONLL file at {self.conll_path}\")\n",
    "            print(f\"Total tokens: {len(conll_lines)-len(df)}\")  # Subtract empty lines\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error converting to CONLL: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def full_pipeline(self, sample_size=50):\n",
    "        \"\"\"Run the complete pipeline from preparation to CONLL conversion\"\"\"\n",
    "        print(\"Starting Amharic NER processing pipeline...\")\n",
    "        \n",
    "        if not self.prepare_labeling_data(sample_size):\n",
    "            return False\n",
    "        \n",
    "        if not self.convert_to_conll():\n",
    "            return False\n",
    "        \n",
    "        print(\"\\nPipeline completed successfully!\")\n",
    "        return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = AmharicNERProcessor()\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    success = processor.full_pipeline(50)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nFinal outputs:\")\n",
    "        print(f\"- Labeled data: {processor.labeling_path}\")\n",
    "        print(f\"- CONLL format: {processor.conll_path}\")\n",
    "    else:\n",
    "        print(\"\\nProcessing failed. Please check error messages.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
