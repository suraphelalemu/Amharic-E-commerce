{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d1f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 3235.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating xlm-roberta-base ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\surap\\AppData\\Local\\Temp\\ipykernel_21392\\2096270160.py:153: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surap\\OneDrive\\Desktop\\10Acadamy\\Amharic-E-commerce\\myenv\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating bert-base-multilingual-cased ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surap\\OneDrive\\Desktop\\10Acadamy\\Amharic-E-commerce\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\surap\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\surap\\OneDrive\\Desktop\\10Acadamy\\Amharic-E-commerce\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 714.29 MB. The target location C:\\Users\\surap\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased\\blobs only has 214.18 MB free disk space.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating bert-base-multilingual-cased: [Errno 28] No space left on device\n",
      "\n",
      "=== Evaluating Davlan/bert-base-multilingual-cased-ner-hrl ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surap\\OneDrive\\Desktop\\10Acadamy\\Amharic-E-commerce\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location C:\\Users\\surap\\.cache\\huggingface\\hub\\models--Davlan--bert-base-multilingual-cased-ner-hrl\\blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "c:\\Users\\surap\\OneDrive\\Desktop\\10Acadamy\\Amharic-E-commerce\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\surap\\.cache\\huggingface\\hub\\models--Davlan--bert-base-multilingual-cased-ner-hrl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\surap\\OneDrive\\Desktop\\10Acadamy\\Amharic-E-commerce\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 1.00 MB. The target location C:\\Users\\surap\\.cache\\huggingface\\hub\\models--Davlan--bert-base-multilingual-cased-ner-hrl\\blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "c:\\Users\\surap\\OneDrive\\Desktop\\10Acadamy\\Amharic-E-commerce\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 709.11 MB. The target location C:\\Users\\surap\\.cache\\huggingface\\hub\\models--Davlan--bert-base-multilingual-cased-ner-hrl\\blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating Davlan/bert-base-multilingual-cased-ner-hrl: [Errno 28] No space left on device\n",
      "\n",
      "=== Evaluating afro-xlmr-base ===\n",
      "Error evaluating afro-xlmr-base: afro-xlmr-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "\n",
      "Model Comparison Results:\n",
      "                                                   f1 precision    recall  \\\n",
      "xlm-roberta-base                             0.005587  0.002976  0.045455   \n",
      "bert-base-multilingual-cased                     None      None      None   \n",
      "Davlan/bert-base-multilingual-cased-ner-hrl      None      None      None   \n",
      "afro-xlmr-base                                   None      None      None   \n",
      "\n",
      "                                            accuracy  speed  \\\n",
      "xlm-roberta-base                             0.01462  2.192   \n",
      "bert-base-multilingual-cased                    None   None   \n",
      "Davlan/bert-base-multilingual-cased-ner-hrl     None   None   \n",
      "afro-xlmr-base                                  None   None   \n",
      "\n",
      "                                                                                         error  \n",
      "xlm-roberta-base                                                                           NaN  \n",
      "bert-base-multilingual-cased                                [Errno 28] No space left on device  \n",
      "Davlan/bert-base-multilingual-cased-ner-hrl                 [Errno 28] No space left on device  \n",
      "afro-xlmr-base                               afro-xlmr-base is not a local folder and is no...  \n",
      "\n",
      "Results saved to model_comparison_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model_comparison.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_prepare_data(conll_path):\n",
    "    \"\"\"Load and prepare CONLL format data\"\"\"\n",
    "    def parse_conll(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        tokens, labels = [], []\n",
    "        current_tokens, current_labels = [], []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:  # Sentence boundary\n",
    "                if current_tokens:\n",
    "                    tokens.append(current_tokens)\n",
    "                    labels.append(current_labels)\n",
    "                    current_tokens, current_labels = [], []\n",
    "                continue\n",
    "            \n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                current_tokens.append(parts[0])\n",
    "                current_labels.append(parts[1])\n",
    "        \n",
    "        if current_tokens:\n",
    "            tokens.append(current_tokens)\n",
    "            labels.append(current_labels)\n",
    "        \n",
    "        return {'tokens': tokens, 'ner_tags': labels}\n",
    "\n",
    "    # Load and parse data\n",
    "    conll_data = parse_conll(conll_path)\n",
    "    dataset = Dataset.from_dict({\n",
    "        'tokens': conll_data['tokens'],\n",
    "        'ner_tags': conll_data['ner_tags']\n",
    "    })\n",
    "\n",
    "    # Define label mappings\n",
    "    label_list = [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-PRICE\", \"I-PRICE\", \"B-LOC\", \"I-LOC\"]\n",
    "    label2id = {l: i for i, l in enumerate(label_list)}\n",
    "    id2label = {i: l for i, l in enumerate(label_list)}\n",
    "\n",
    "    return dataset, label_list, label2id, id2label\n",
    "\n",
    "def tokenize_and_align(dataset, tokenizer, label2id):\n",
    "    \"\"\"Tokenize and align labels with tokens\"\"\"\n",
    "    def tokenize_and_align_labels(examples):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[\"tokens\"],\n",
    "            truncation=True,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            \n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label2id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            \n",
    "            labels.append(label_ids)\n",
    "        \n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    return dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "def get_compute_metrics(label_list):\n",
    "    \"\"\"Return metrics computation function\"\"\"\n",
    "    seqeval = evaluate.load(\"seqeval\")\n",
    "    \n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        true_predictions = [\n",
    "            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n",
    "    \n",
    "    return compute_metrics\n",
    "\n",
    "def compare_models(conll_path, models_to_compare):\n",
    "    \"\"\"Compare multiple NER models\"\"\"\n",
    "    # Load and prepare data\n",
    "    dataset, label_list, label2id, id2label = load_and_prepare_data(conll_path)\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    tokenized_datasets = tokenize_and_align(dataset, AutoTokenizer.from_pretrained(\"xlm-roberta-base\"), label2id)\n",
    "    split_datasets = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"../comparison_results\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=None\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForTokenClassification(AutoTokenizer.from_pretrained(\"xlm-roberta-base\"))\n",
    "    \n",
    "    # Compute metrics\n",
    "    compute_metrics = get_compute_metrics(label_list)\n",
    "    \n",
    "    # Compare models\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models_to_compare:\n",
    "        print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "        \n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                model_name,\n",
    "                num_labels=len(label_list),\n",
    "                id2label=id2label,\n",
    "                label2id=label2id\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                eval_dataset=split_datasets[\"test\"],\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "            \n",
    "            metrics = trainer.evaluate()\n",
    "            results[model_name] = {\n",
    "                'f1': metrics['eval_f1'],\n",
    "                'precision': metrics['eval_precision'],\n",
    "                'recall': metrics['eval_recall'],\n",
    "                'accuracy': metrics['eval_accuracy'],\n",
    "                'speed': metrics['eval_runtime']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "            results[model_name] = {\n",
    "                'f1': None,\n",
    "                'precision': None,\n",
    "                'recall': None,\n",
    "                'accuracy': None,\n",
    "                'speed': None,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return pd.DataFrame(results).T.sort_values('f1', ascending=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    CONLL_PATH = \"../CoNLL/amharic_ner.conll\"  # Update with your path\n",
    "    MODELS_TO_COMPARE = [\n",
    "        \"xlm-roberta-base\",\n",
    "        \"bert-base-multilingual-cased\",\n",
    "        \"Davlan/bert-base-multilingual-cased-ner-hrl\",\n",
    "        \"afro-xlmr-base\"\n",
    "    ]\n",
    "    \n",
    "    # Run comparison\n",
    "    results_df = compare_models(CONLL_PATH, MODELS_TO_COMPARE)\n",
    "    \n",
    "    # Save and display results\n",
    "    print(\"\\nModel Comparison Results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    results_df.to_csv(\"../comparison_result/model_comparison_results.csv\")\n",
    "    print(\"\\nResults saved to model_comparison_results.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
